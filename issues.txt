

1. Initial Attempt: Using Headless Chromium

-I started by using headless Chromium via command-line to download the DOM of target web pages. The following flags were used:
--headless --disable-gpu --no-sandbox --disable-dev-shm-usage --dump-dom 
- Websites like lokmat.com returned only the static HTML skeleton without any actual article or news content.
- Some websites blocked headless Chromium entirely or returned CAPTCHA challenges.
- Even after introducing virtual wait (timeouts before dumping DOM), dynamic content did not render correctly.
- Repeated requests led to temporary IP bans or blacklisting, especially after multiple automated fetches.
- Some pages required cookies or JavaScript-based tokens to load.
- DOM was dumped before asynchronous content had time to finish loading.


2. Second Attempt: Using Playwright in Headless Mode

- I moved to Playwright for better support of JavaScript rendering and realistic browser automation.
- Playwright worked well for marathi.indiatimes.com â€” the full article and dynamic elements loaded as expected.
- However, on websites like lokmat.com and pudhari.news, the same issues reappeared:
- Only raw HTML was returned.
- Article content was either partially loaded or completely missing.
- After multiple runs, the website started blocking the browser session, likely due to bot detection.
- Even when JavaScript executed correctly, content was hidden behind pop-ups, cookie banners, or scroll-triggered load events.
- Some elements were loaded using lazy loading or external APIs, which Playwright did not wait for by default.
- Playwright is heavier and slower than other approaches, especially when scraping a large number of pages.


3. General Challenges Across All Tools

While testing multiple approaches, the following general challenges were observed:

- Every website has a unique structure, requiring custom parsing logic.
- Websites use anti-bot mechanisms like:
- Monitoring behavior patterns (mouse movements, scroll events)
- Detecting headless browser fingerprinting
- Blocking unusual user-agent headers
- Dynamic content often depends on:
  - Scroll events (infinite scroll)
  - Post-load API calls
  - Client-side scripts that render after delay
- Use of CAPTCHA or JavaScript-based cookie verifications prevents direct automated access.

4. Final Approach: Using Requests + BeautifulSoup with User-Agent Headers

-To simplify and stabilize the process, I implemented a fallback approach using the requests library combined with BeautifulSoup for parsing.

- Added realistic browser User-Agent headers:
  headers = {"User-Agent": "Mozilla/5.0"}
- Switched parser dynamically based on the site content type (HTML or XML).
- Some websites like bhaskar.com or pudhari.news offer partial or full content via XML feeds (RSS, Atom), so I included XML parsing where applicable.
- For static pages, this method was lightweight and effective.
- Easy to write custom logic to extract titles, article bodies, publication dates, etc.
- Works well when no JavaScript rendering is required.


# Summary 

- Headless Chromium is fast but easily detected by websites and does not handle JavaScript rendering well.
- Playwright offers more flexibility but still has limits without proper wait handling, scroll emulation, or stealth setup.
- Static HTML scraping using BeautifulSoup is reliable only when JavaScript is not critical to the content.
- Each website requires individual handling due to structural and behavioral differences.



